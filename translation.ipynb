{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d2cb5a-c9ea-4ffd-81f9-d6a6d148b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline \n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad10c6-b66e-48da-a5c2-f7645bffcdcf",
   "metadata": {},
   "source": [
    "Define SOS, EOS, Length, Device and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd6f5857-d773-4e84-aeb7-270fe459c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS = 0\n",
    "EOS = 1\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "def convert_unicode_ascii(unicode_str):\n",
    "    return ''.join(char for char in unicodedata.normalize('NFD', unicode_str) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = convert_unicode_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) #remove punctuation\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) #remove non letter chars\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7466ac-78e4-4b90-ba17-0f695c06ae3b",
   "metadata": {},
   "source": [
    "Language Class. We can then manage Integer/String mappings. We need to do this for both languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a3f0c87-e1de-4c5b-96c5-b99ac71bcb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageProcessor:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.stoi = {} #String to Integer\n",
    "        self.itos = {0: \"SOS\", 1: \"EOS\"} #Integer to String \n",
    "        self.wordFrequency = {} #Word Frequency Mapping\n",
    "        self.num_words = 2\n",
    "\n",
    "    def index_word(self,word):\n",
    "        if word not in self.stoi:\n",
    "            #if not found, create mappings\n",
    "            self.stoi[word] = self.num_words #unique index for each new word\n",
    "            self.wordFrequency[word] = 1\n",
    "            self.itos[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            #if found, increment cout\n",
    "            self.stoi[word] += 1\n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd274302-5bad-4029-a2d7-fd80a9f34f88",
   "metadata": {},
   "source": [
    "Process the Data and prepare it using word pairings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220077b5-d3e8-44f0-a3d5-f0051777fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 229803 sentence pairs\n",
      "Trimmed pair set to 0 sentence pairs\n",
      "Error: No pairs left after filtering.\n",
      "Data processing failed.\n"
     ]
    }
   ],
   "source": [
    "def process_languages(lang1, lang2, reverse=False):\n",
    "    file_path = 'fra.txt'\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return None, None, None\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.read().strip().split('\\n')\n",
    "    except IOError as e:\n",
    "        print(f\"Error reading file '{file_path}': {e}\")\n",
    "        return None, None, None\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Error decoding file '{file_path}': {e}\")\n",
    "        print(\"Try specifying a different encoding.\")\n",
    "        return None, None, None\n",
    "\n",
    "    if not lines:\n",
    "        print(f\"Error: File '{file_path}' is empty.\")\n",
    "        return None, None, None\n",
    "\n",
    "    try:\n",
    "        pairs = [[normalize_string(s) for s in line.split('\\t')] for line in lines]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lines: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    if reverse:\n",
    "        #if reverse translation, reverse the orders\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        inputLang = LanguageProcessor(lang2)\n",
    "        outputLang = LanguageProcessor(lang1)\n",
    "    else:\n",
    "        inputLang = LanguageProcessor(lang1)\n",
    "        outputLang = LanguageProcessor(lang2)\n",
    "    \n",
    "    return inputLang, outputLang, pairs\n",
    "\n",
    "# def filter_pair(p):\n",
    "#     good_prefixes = (\n",
    "#         \"i am \", \"i m \",\n",
    "#         \"he is \", \"he s \",\n",
    "#         \"she is \", \"she s \",\n",
    "#         \"you are \", \"you re \"\n",
    "#     )\n",
    "#     return (len(p[0].split(' ')) < MAX_LENGTH and \n",
    "#             len(p[1].split(' ')) < MAX_LENGTH and \n",
    "#             any(p[1].lower().startswith(prefix) for prefix in good_prefixes))\n",
    "\n",
    "# def filter_pairs(pairs):\n",
    "#     return [pair for pair in pairs if filter_pair(pair)]\n",
    "\n",
    "def process_data(lang1, lang2, reverse=False):\n",
    "    inputLang, outputLang, pairs = process_languages(lang1, lang2, reverse)\n",
    "    if pairs is None:\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = pairs[:50000]\n",
    "    print(f\"Trimmed pair set to {len(pairs)} sentence pairs\")\n",
    "\n",
    "    if not pairs:\n",
    "        print(\"Error: No pairs left after filtering.\")\n",
    "        return None, None, None\n",
    "\n",
    "    for p in tqdm(pairs, desc=\"Indexing words\"):\n",
    "        inputLang.index_words(p[0]) #add first index of pair to input Lang\n",
    "        outputLang.index_words(p[1]) #add second index of pair to output Lang\n",
    "    print(\"Indexed words:\")\n",
    "    print(f\"{inputLang.name}: {inputLang.num_words}\")\n",
    "    print(f\"{outputLang.name}: {outputLang.num_words}\")\n",
    "    return inputLang, outputLang, pairs\n",
    "\n",
    "inputLang, outputLang, pairs = process_data('eng', 'fra', True)\n",
    "if pairs:\n",
    "    print(random.choice(pairs))\n",
    "else:\n",
    "    print(\"Data processing failed.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3070ea-f9ff-47e3-8b36-14ebdb3bc895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d5b79-2727-413b-8b3f-a91960936bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
